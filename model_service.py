# model_service.py
import pandas as pd
import numpy as np
import joblib
import requests
import os
import re
import warnings
import json

# ==========================================
# ğŸš‘ [è¨­å®š] æŠ‘åˆ¶è­¦å‘Š & ç›¸å®¹æ€§è¨­å®š
# ==========================================
os.environ["CUDA_VISIBLE_DEVICES"] = "-1"
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"
os.environ["TF_ENABLE_ONEDNN_OPTS"] = "0"
warnings.simplefilter(action='ignore', category=FutureWarning)
warnings.simplefilter(action='ignore', category=UserWarning) # å¿½ç•¥æ—¥æœŸè§£æè­¦å‘Š

from datetime import datetime, timedelta
import tensorflow as tf
from tensorflow import keras

# ==========================================
# âš™ï¸ è¨­å®šèˆ‡å¸¸æ•¸
# ==========================================
MODEL_FILES = {
    "lgbm": "lgbm_model.pkl",
    "lstm": "lstm_model.keras",
    "scaler_seq": "scaler_seq.pkl",
    "scaler_dir": "scaler_dir.pkl",
    "scaler_target": "scaler_target.pkl",
    "weights": "ensemble_weights.pkl",
    "history_data": "final_training_data_with_humidity.csv"
}

LIVE_DATA_URL = "https://getpantry.cloud/apiv1/pantry/6e282296-e38a-454b-9895-a86d12a82731/basket/new"
HISTORY_PANTRY_ID = "6a2e85f5-4af4-4efd-bb9f-c5604fe8475e" 
LOOKBACK_HOURS = 168

# ==========================================
# ğŸ”§ [æ–°å¢] è¼”åŠ©å·¥å…·ï¼šè‡ªå‹•è¨ˆç®—ç±ƒå­åç¨±
# ==========================================
def get_basket_name(dt: datetime):
    """
    æ ¹æ“šæ—¥æœŸè‡ªå‹•è¨ˆç®— Basket åç¨±ï¼Œä¾‹å¦‚: 2026-01-06 -> '2026-q1'
    """
    quarter = (dt.month - 1) // 3 + 1
    return f"{dt.year}-q{quarter}"

# ==========================================
# ğŸ› ï¸ ç‰¹å¾µå·¥ç¨‹ (ä¿æŒä¸è®Š)
# ==========================================
def get_taiwan_holidays():
    return ["2024-01-01", "2024-02-08", "2024-02-09", "2024-02-10", "2024-02-11", 
            "2024-02-12", "2024-02-13", "2024-02-14", "2024-02-28", "2024-04-04", 
            "2024-04-05", "2024-05-01", "2024-06-10", "2024-09-17", "2024-10-10",
            # å»ºè­°è£œä¸Š 2025, 2026 çš„å‡æœŸï¼Œæˆ–æ˜¯è®“å®ƒè‡ªå‹•åŒ–ï¼Œç›®å‰å…ˆç¶­æŒåŸæ¨£
            "2025-01-01", "2025-01-28", "2025-01-29", "2025-01-30", "2025-01-31"]

def add_lgbm_features(df):
    df = df.copy()
    df["hour"] = df.index.hour
    df["day_of_week"] = df.index.dayofweek
    df["month"] = df.index.month
    tw_holidays = get_taiwan_holidays()
    date_strs = df.index.strftime("%Y-%m-%d")
    df["is_holiday_or_weekend"] = ((df["day_of_week"] >= 5) | (date_strs.isin(tw_holidays))).astype(int)
    df["is_weekend"] = df["is_holiday_or_weekend"]
    df["hour_sin"] = np.sin(2 * np.pi * df["hour"] / 24.0)
    df["hour_cos"] = np.cos(2 * np.pi * df["hour"] / 24.0)
    df["week_sin"] = np.sin(2 * np.pi * df["day_of_week"] / 7.0)
    df["week_cos"] = np.cos(2 * np.pi * df["day_of_week"] / 7.0)
    df["month_sin"] = np.sin(2 * np.pi * df["month"] / 12.0)
    df["month_cos"] = np.cos(2 * np.pi * df["month"] / 12.0)
    peak_hours = [10, 11, 12, 13, 14, 15, 17, 18, 19, 20]
    df["is_peak"] = df["hour"].isin(peak_hours).astype(int)
    for lag in [24, 168, 720]:
         if f'lag_{lag}h' not in df.columns: df[f'lag_{lag}h'] = df["power"].shift(lag)
    for i in [1, 2, 3]:
        if f'temp_lag_{i}' not in df.columns: df[f'temp_lag_{i}'] = df["temperature"].shift(i)
    for window_days in [7, 14, 30]:
        window_hours = window_days * 24
        df[f'ma_{window_days}d'] = df["power"].shift(1).rolling(window=window_hours, min_periods=1).mean()
        df[f'std_{window_days}d'] = df["power"].shift(1).rolling(window=window_hours, min_periods=1).std()
    df["temp_x_peak"] = df["temperature"] * df["is_peak"]
    df["temp_squared"] = df["temperature"] ** 2
    return df

def add_lstm_features(df):
    df = df.copy()
    df["hour"] = df.index.hour.astype(float)
    tw_holidays = get_taiwan_holidays()
    date_strs = df.index.strftime("%Y-%m-%d")
    df["is_weekend"] = ((df.index.dayofweek >= 5) | (date_strs.isin(tw_holidays))).astype(float)
    df["day_of_week"] = df.index.dayofweek.astype(float)
    df["hour_sin"] = np.sin(2 * np.pi * df["hour"] / 24.0)
    df["hour_cos"] = np.cos(2 * np.pi * df["hour"] / 24.0)
    df["week_sin"] = np.sin(2 * np.pi * df["day_of_week"] / 7.0)
    df["week_cos"] = np.cos(2 * np.pi * df["day_of_week"] / 7.0)
    df["temp_squared"] = df["temperature"] ** 2
    df["lag_24h"] = df["power"].shift(24)
    df["lag_168h"] = df["power"].shift(168)
    df["rolling_mean_24h_safe"] = df["power"].shift(24).rolling(window=24, min_periods=1).mean()
    df["rolling_std_24h_safe"] = df["power"].shift(24).rolling(window=24, min_periods=1).std()
    df["rolling_mean_168h"] = df["power"].shift(24).rolling(window=168, min_periods=1).mean()
    df["rolling_std_168h"] = df["power"].shift(24).rolling(window=168, min_periods=1).std()
    return df

# ==========================================
# ğŸ“¥ è³‡æ–™ç²å–é‚è¼¯
# ==========================================
def find_data_list(data_dict):
    target_key = "listAMIBase15MinData"
    if target_key in data_dict:
        return data_dict[target_key], None
    for key, value in data_dict.items():
        date_match = re.match(r"^\d{4}-\d{2}-\d{2}$", str(key))
        current_date = key if date_match else None
        if isinstance(value, dict):
            found, sub_date = find_data_list(value)
            if found: return found, (sub_date if sub_date else current_date)
        if isinstance(value, list) and len(value) > 0 and current_date:
            if isinstance(value[0], dict) and ("power" in value[0] or "power_kW" in value[0]):
                return value, current_date
    return None, None

def process_raw_data_to_df(target_list, date_context):
    if not target_list:
        return pd.DataFrame()

    df = pd.DataFrame(target_list)
    
    if 'power' in df.columns:
        df = df.rename(columns={'power': 'power_kW'})
    
    try:
        if 'full_timestamp' in df.columns:
            df['timestamp'] = pd.to_datetime(df['full_timestamp'], errors='coerce')
        elif 'date' in df.columns and 'time' in df.columns:
            df['timestamp'] = pd.to_datetime(df['date'].astype(str) + " " + df['time'].astype(str), errors='coerce')
        elif 'time' in df.columns:
            if date_context:
                df['timestamp'] = pd.to_datetime(f"{date_context} " + df['time'], errors='coerce')
            else:
                df['timestamp'] = pd.to_datetime(df['time'], errors='coerce')
        else:
            return pd.DataFrame() 
            
    except Exception as e:
        print(f"âš ï¸ æ™‚é–“è§£æå¤±æ•—: {e}")
        return pd.DataFrame()

    if 'timestamp' not in df.columns or 'power_kW' not in df.columns:
        return pd.DataFrame()

    df = df.dropna(subset=['timestamp'])
    df = df.set_index('timestamp').sort_index()
    df['power_kW'] = pd.to_numeric(df['power_kW'], errors='coerce')
    
    if 'isMissingData' in df.columns:
        df.loc[df['isMissingData'] == 1, 'power_kW'] = np.nan
        df.loc[df['isMissingData'] == '1', 'power_kW'] = np.nan
    
    df['power_kW'] = df['power_kW'].replace(0, np.nan)
    df['power_kW'] = df['power_kW'].replace(0.0, np.nan)
    df['power_kW'] = df['power_kW'].ffill().bfill()
    
    if 'temperature' not in df.columns:
        df['temperature'] = 25.0
        df['humidity'] = 70.0
        
    return df[['power_kW', 'temperature', 'humidity']]

def fetch_live_data():
    try:
        response = requests.get(LIVE_DATA_URL, timeout=5)
        data_json = response.json()
        
        if data_json.get('status') != 1:
            print(f"âš ï¸ [Live] API Status: {data_json.get('status')} (æš«ç„¡å³æ™‚è³‡æ–™)")
            return None
            
        raw_data = data_json['data']
        if isinstance(raw_data, list) and len(raw_data) > 0:
            first_item = raw_data[0]
            if isinstance(first_item, dict) and ("power" in first_item or "power_kW" in first_item):
                print(f"âœ… [Live] å–å¾—æ•£è£è³‡æ–™ {len(raw_data)} ç­†")
                return process_raw_data_to_df(raw_data, None)

        target_list = []
        date_context = None
        if isinstance(raw_data, list) and len(raw_data) > 0:
            target_list, date_context = find_data_list(raw_data[0])
        elif isinstance(raw_data, dict):
            target_list, date_context = find_data_list(raw_data)
        
        if target_list:
            print(f"âœ… [Live] è§£åŒ…æˆåŠŸ (Date: {date_context})")
            return process_raw_data_to_df(target_list, date_context)
            
        return None
    except:
        return None

def fetch_recent_history_gap():
    """
    [ä¿®æ­£] è‡ªå‹•åˆ¤æ–·è¦æŠ“å–çš„æ­·å²ç±ƒå­ (è·¨å­£åº¦æ”¯æ´)
    """
    now = datetime.now()
    
    # 1. å–å¾—æœ¬å­£ç±ƒå­ (ä¾‹å¦‚ 2026-q1)
    current_basket = get_basket_name(now)
    
    # 2. å–å¾—ä¸Šä¸€å­£ç±ƒå­ (ä¾‹å¦‚ 2025-q4)
    # é‚è¼¯ï¼šå–å¾—æœ¬æœˆ1è™Ÿçš„å‰ä¸€å¤©ï¼Œå°±æ˜¯ä¸Šå€‹æœˆåº•ï¼Œç”¨é‚£ä¸€å¤©ä¾†ç®—ä¸Šä¸€å­£
    last_month_date = now.replace(day=1) - timedelta(days=1)
    prev_basket = get_basket_name(last_month_date)
    
    # 3. å»ºç«‹ç›®æ¨™æ¸…å–® (å»é‡æ’åº)
    target_baskets = sorted(list({prev_basket, current_basket}))
    
    all_gap_dfs = []
    
    print(f"â³ [Gap] æ­£åœ¨è£œé½Šæ­·å²è³‡æ–™ï¼Œç›®æ¨™ç±ƒå­: {target_baskets} ...")
    
    for basket in target_baskets:
        url = f"https://getpantry.cloud/apiv1/pantry/{HISTORY_PANTRY_ID}/basket/{basket}"
        try:
            r = requests.get(url, timeout=5)
            if r.status_code == 200:
                data = r.json()
                if "data" in data and isinstance(data["data"], list):
                    raw_items = data["data"]
                    print(f"   ğŸ“¦ [Gap] {basket} ä¸‹è¼‰æˆåŠŸ: {len(raw_items)} items")
                    
                    if len(raw_items) > 0 and isinstance(raw_items[0], dict) and ("power" in raw_items[0] or "power_kW" in raw_items[0]):
                         # print("   ğŸ” [Gap] åµæ¸¬åˆ°æ•£è£æ ¼å¼ï¼Œä½¿ç”¨å…§å»ºæ—¥æœŸæ¬„ä½è§£æ...")
                         df = process_raw_data_to_df(raw_items, None)
                         if not df.empty:
                            all_gap_dfs.append(df)
                    else:
                        for item in raw_items:
                            target_list, date_context = find_data_list(item)
                            if target_list:
                                sub_df = process_raw_data_to_df(target_list, date_context)
                                if not sub_df.empty:
                                    all_gap_dfs.append(sub_df)
        except Exception as e:
            print(f"   âš ï¸ [Gap Error] {basket}: {e}")
    
    if not all_gap_dfs:
        print("   âš ï¸ [Gap] æœªèƒ½è£œå…¥ä»»ä½•æœ‰æ•ˆè³‡æ–™")
        return pd.DataFrame()
        
    try:
        full_gap_df = pd.concat(all_gap_dfs)
        full_gap_df = full_gap_df.sort_index()
        full_gap_df = full_gap_df[~full_gap_df.index.duplicated(keep='first')]
        print(f"   âœ… [Gap] è£œæ´å®Œæˆï¼å…± {len(full_gap_df)} ç­† (ç¯„åœ: {full_gap_df.index.min()} ~ {full_gap_df.index.max()})")
        return full_gap_df
    except:
        return pd.DataFrame()

# ==========================================
# ğŸ’¾ [è‡ªå‹•æ­¸æª”] æ ¸å¿ƒåŠŸèƒ½ (åŒ…å«æ»‘å‹•è¦–çª—)
# ==========================================
def auto_archive_live_data(live_df):
    """
    è‡ªå‹•å°‡æœ€æ–°çš„ Live Data æ­¸æª”åˆ°å°æ‡‰çš„æ­·å² Pantry Basket
    åŒ…å«ï¼šè‡ªå‹•åˆ¤æ–·å­£åº¦ã€æ»‘å‹•è¦–çª—(ä¿ç•™ç´„2000ç­†)ã€é˜²é‡è¤‡æª¢æŸ¥
    """
    if live_df is None or live_df.empty:
        return

    try:
        # 1. å–å‡º Live Data æœ€æ–°çš„ä¸€ç­†è³‡æ–™
        latest_record = live_df.iloc[-1].copy()
        
        # ğŸ”¥ [ä¿®æ­£] å‹•æ…‹æ±ºå®šç›®æ¨™ç±ƒå­
        dynamic_basket = get_basket_name(latest_record.name)
        history_url = f"https://getpantry.cloud/apiv1/pantry/{HISTORY_PANTRY_ID}/basket/{dynamic_basket}"
        
        new_data_payload = {
            "date": latest_record.name.strftime("%Y-%m-%d"),
            "time": latest_record.name.strftime("%H:%M:%S"),
            "power_kW": float(latest_record['power_kW']),
            "temperature": float(latest_record['temperature']),
            "humidity": float(latest_record['humidity'])
        }

        print(f"ğŸ’¾ [Archive] æº–å‚™æ­¸æª”åˆ° [{dynamic_basket}]: {new_data_payload['date']} {new_data_payload['time']}")

        # 2. ä¸‹è¼‰ç›®å‰çš„æ­·å²è³‡æ–™
        headers = {'Content-Type': 'application/json'}
        r = requests.get(history_url, timeout=10)
        
        current_history = []
        if r.status_code == 200:
            data = r.json()
            if "data" in data and isinstance(data["data"], list):
                current_history = data["data"]
            elif isinstance(data, list):
                current_history = data
            elif isinstance(data, dict):
                current_history = [data]
        
        # 3. é‡è¤‡æª¢æŸ¥ (é˜²å‘†)
        if current_history:
            last_item = current_history[-1]
            last_date = last_item.get('date', '')
            last_time = last_item.get('time', '')
            
            # åªæ¯”å°åˆ°åˆ†é˜ (å‰5ç¢¼) ä»¥å®¹å¿ç§’æ•¸å·®ç•°
            new_time_short = new_data_payload['time'][:5]
            last_time_short = str(last_time)[:5]
            
            if last_date == new_data_payload['date'] and last_time_short == new_time_short:
                print("   âš ï¸ [Archive] è³‡æ–™å·²å­˜åœ¨ (æ™‚é–“é‡è¤‡)ï¼Œè·³éå­˜æª”ã€‚")
                return

        # 4. é™„åŠ æ–°è³‡æ–™
        current_history.append(new_data_payload)
        
        # 5. æ»‘å‹•è¦–çª—ä¿è­· (é¿å… Basket çˆ†æ»¿)
        MAX_RECORDS = 2000
        if len(current_history) > MAX_RECORDS:
            cut_count = len(current_history) - MAX_RECORDS
            current_history = current_history[-MAX_RECORDS:]
            print(f"   âœ‚ï¸ [Archive] è§¸ç™¼æ»‘å‹•è¦–çª—ï¼Œç§»é™¤èˆŠè³‡æ–™ {cut_count} ç­†")

        # 6. POST å›å»
        payload_to_send = {"data": current_history}
        post_r = requests.post(history_url, json=payload_to_send, headers=headers, timeout=10)
        
        if post_r.status_code == 200:
            print(f"   âœ… [Archive] æ­¸æª”æˆåŠŸï¼ç›®å‰æ­·å²ç­†æ•¸: {len(current_history)}")
        else:
            print(f"   âŒ [Archive] ä¸Šå‚³å¤±æ•—: {post_r.text}")

    except Exception as e:
        print(f"   âŒ [Archive Error] æ­¸æª”éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")

# ==========================================
# ğŸ§  ä¸»é æ¸¬æµç¨‹
# ==========================================
def load_resources_and_predict():
    resources = {}
    try:
        # 1. è¼‰å…¥æ¨¡å‹
        resources['lgbm'] = joblib.load(MODEL_FILES['lgbm'])
        resources['lstm'] = keras.models.load_model(MODEL_FILES['lstm'])
        resources['scaler_seq'] = joblib.load(MODEL_FILES['scaler_seq'])
        resources['scaler_dir'] = joblib.load(MODEL_FILES['scaler_dir'])
        resources['scaler_target'] = joblib.load(MODEL_FILES['scaler_target'])
        resources['weights'] = joblib.load(MODEL_FILES['weights'])
        
        # 2. æº–å‚™ä¸‰ä»½æ•¸æ“š
        print("ğŸ“¥ æ­£åœ¨æ•´åˆä¸‰æ–¹æ•¸æ“šæº...")
        
        # (A) éœæ…‹ CSV
        hist_df = pd.read_csv(MODEL_FILES['history_data'])
        hist_df['datetime'] = pd.to_datetime(hist_df['datetime'])
        hist_df = hist_df.set_index('datetime').sort_index()
        if 'power' in hist_df.columns: hist_df = hist_df.rename(columns={'power': 'power_kW'})
        print(f"   ğŸ“„ [CSV] éœæ…‹è³‡æ–™: åˆ° {hist_df.index.max()}")
        
        # (B) é›²ç«¯è£œæ´ (å·²æ”¯æ´è·¨å­£åº¦)
        gap_df = fetch_recent_history_gap()
        
        # (C) å³æ™‚ Live (å…è¨±å¤±æ•—)
        live_df = fetch_live_data()
        if live_df is None: 
            print("   âš ï¸ [Live] æš«ç„¡å³æ™‚è³‡æ–™ï¼Œä½¿ç”¨æ­·å²æ¨ä¼°")
            live_df = pd.DataFrame()
        
        # 3. å¤§åˆä½µ
        dfs_to_concat = [df for df in [hist_df, gap_df, live_df] if not df.empty]
        if not dfs_to_concat: return None, None

        combined_df = pd.concat(dfs_to_concat)
        combined_df = combined_df[~combined_df.index.duplicated(keep='last')].sort_index()
        combined_df['power'] = combined_df['power_kW']
        
        print(f"ğŸ‰ [Total] æ•´åˆå®Œç•¢ï¼æœ€æ–°æ™‚é–“: {combined_df.index.max()}")

        # 4. é æ¸¬
        buffer_size = 2000
        df_ready = combined_df.iloc[-buffer_size:].copy()
        
        if pd.isna(df_ready.iloc[-1]['power']) or df_ready.iloc[-1]['power'] == 0:
             valid_idx = df_ready['power'].last_valid_index()
             if valid_idx:
                 df_ready = df_ready.loc[:valid_idx]
        
        last_time = df_ready.index[-1]
        future_dates = [last_time + timedelta(hours=i+1) for i in range(24)]
        future_df = pd.DataFrame(index=future_dates, columns=df_ready.columns)
        
        future_df['temperature'] = df_ready['temperature'].iloc[-1]
        future_df['humidity'] = df_ready['humidity'].iloc[-1]
        
        full_context = pd.concat([df_ready, future_df])
        
        df_lgbm = add_lgbm_features(full_context)
        df_lstm = add_lstm_features(full_context)
        
        target_feat_lgbm = df_lgbm.iloc[-24:]
        target_feat_lstm = df_lstm.iloc[-24:]
        
        lgbm_feature_names = resources['lgbm'].feature_name()
        X_lgbm = target_feat_lgbm[lgbm_feature_names]
        pred_lgbm = resources['lgbm'].predict(X_lgbm)
        
        current_idx = -25
        seq_cols = ["power", "temperature", "humidity", "hour_sin", "hour_cos", "is_weekend"]
        dir_cols = ["lag_24h", "lag_168h", "temperature", "humidity", "hour_sin", "hour_cos", "week_sin", "week_cos", "is_weekend", "temp_squared", "rolling_mean_24h_safe", "rolling_std_24h_safe", "rolling_mean_168h", "rolling_std_168h"]
        
        seq_data = df_lstm[seq_cols].iloc[current_idx-LOOKBACK_HOURS+1 : current_idx+1]
        dir_data = df_lstm[dir_cols].iloc[current_idx+1 : current_idx+2]
        
        X_seq = resources['scaler_seq'].transform(seq_data).reshape(1, LOOKBACK_HOURS, -1)
        X_dir = resources['scaler_dir'].transform(dir_data)
        
        pred_lstm_scaled = resources['lstm'].predict([X_seq, X_dir], verbose=0)
        pred_lstm = resources['scaler_target'].inverse_transform(pred_lstm_scaled).flatten()
        
        pred_final = (pred_lgbm * resources['weights']['w_lgbm']) + (pred_lstm * resources['weights']['w_lstm'])
        
        result_df = pd.DataFrame({
            "æ™‚é–“": future_dates,
            "é æ¸¬å€¼": pred_final,
            "LGBM": pred_lgbm,
            "LSTM": pred_lstm
        }).set_index("æ™‚é–“")
        
        # =========== ğŸ”¥ åŸ·è¡Œè‡ªå‹•æ­¸æª” ===========
        if live_df is not None and not live_df.empty:
            print("ğŸš€ [System] å•Ÿå‹•èƒŒæ™¯æ­¸æª”ç¨‹åº...")
            auto_archive_live_data(live_df) # ä¸ç”¨åƒæ•¸ï¼Œè‡ªå‹•åˆ¤æ–·
        # =====================================
        
        return result_df, combined_df
        
    except Exception as e:
        print(f"âŒ [Model Service Error]: {e}")
        return None, None